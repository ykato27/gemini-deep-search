"""
Phase 1: äº‹ä¾‹æ¤œç´¢ãƒ»ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
LangGraph/ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦Webã‹ã‚‰æœ€æ–°è¨˜äº‹ã‚’åé›†ã—ã€
æ§‹é€ åŒ–ã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
"""
import os
import sys
import time
import json
import traceback
from datetime import datetime, timedelta

from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_tavily import TavilySearch
from langgraph.prebuilt import create_react_agent

# JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (åˆ†æãƒ•ã‚§ãƒ¼ã‚ºã¨å…±æœ‰)
RESEARCH_DATA_PATH = "reports/research_data.json"

def search_and_extract_data(target_year: int = None):
    """
    é€±æ¬¡èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã‚’Webæ¤œç´¢ã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸJSONã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
    """
    print("\n" + "=" * 60)
    print("ğŸš€ Phase 1: äº‹ä¾‹æ¤œç´¢ã¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’é–‹å§‹")
    print("=" * 60)

    # --- 1. ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª ---
    google_api_key = os.environ.get("GOOGLE_API_KEY")
    tavily_api_key = os.environ.get("TAVILY_API_KEY")

    if not google_api_key or not tavily_api_key:
        print("âŒ ã‚¨ãƒ©ãƒ¼: GOOGLE_API_KEY/TAVILY_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)
    print("âœ“ APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¾ã—ãŸ")

    # --- 2. LLMã¨ãƒ„ãƒ¼ãƒ«ã®æº–å‚™ ---
    model = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0, # æŠ½å‡ºã‚¿ã‚¹ã‚¯ã®ãŸã‚æ¸©åº¦ã¯ä½ãè¨­å®š
    )

    search_tool = TavilySearch(
        max_results=10,
        search_depth="advanced",
        include_raw_content=True,
    )
    tools = [search_tool]
    
    # --- 3. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ ---
    agent_executor = create_react_agent(model, tools)
    print("âœ“ ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®šã—ã¾ã—ãŸ")

    # --- 4. æ¤œç´¢å¯¾è±¡å¹´ã®è¨­å®šã¨æœŸé–“ã®è¨ˆç®— ---
    today = datetime.now()
    start_date = (today - timedelta(days=7)).strftime("%Y-%m-%d")
    year = target_year or today.year
    
    print(f"ğŸ“… æ¤œç´¢å¯¾è±¡å¹´: {year}")
    print(f"ğŸ—“ï¸ æ¤œç´¢é–‹å§‹æ—¥: {start_date} (éå»1é€±é–“)")

    # --- 5. èª¿æŸ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å®šç¾© (æ¤œç´¢/æŠ½å‡ºã«ç‰¹åŒ–) ---
    search_prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªãƒªã‚µãƒ¼ãƒã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã‚ã‚Šã€Webæ¤œç´¢ã¨æƒ…å ±æŠ½å‡ºã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã„ã€æœ€æ–°ã®è¨˜äº‹ã‚’èª¿æŸ»ã—ã€æŠ½å‡ºã—ãŸæƒ…å ±ã‚’**æœ€çµ‚çš„ã«å³å¯†ãªJSONé…åˆ—å½¢å¼**ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# ğŸ¯ ã‚¿ã‚¹ã‚¯
**éå»1é€±é–“ä»¥å†…ï¼ˆ{start_date}ä»¥é™ï¼‰ã«å…¬é–‹ã•ã‚ŒãŸ**ã‚¹ã‚­ãƒ«ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆãƒ»ã‚¿ãƒ¬ãƒ³ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ**ã«é–¢ã™ã‚‹æ¬§ç±³ã®æœ€æ–°è¨˜äº‹ã‚’èª¿æŸ»ã—ã€è©³ç´°ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

# ğŸ” èª¿æŸ»å¯¾è±¡ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
- **å¯¾è±¡åœ°åŸŸ**: ç±³å›½ã€è‹±å›½ã€ãƒ‰ã‚¤ãƒ„ã€ãƒ•ãƒ©ãƒ³ã‚¹ã€ã‚ªãƒ©ãƒ³ãƒ€ã€ã‚¹ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ³ã€ã‚¤ã‚¿ãƒªã‚¢ã€ã‚¹ãƒšã‚¤ãƒ³
- **æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ"past week", "last 7 days", "{year}"ï¼‰ã‚’çµ„ã¿åˆã‚ã›ã¦ã€**å¤šæ§˜ãªæ¤œç´¢ã‚’5å›ä»¥ä¸Šå®Ÿè¡Œ**ã—ã¦ãã ã•ã„ã€‚
  - "skill management", "skills management", "talent management", "competency mapping", "skills taxonomy", "workforce upskilling", "reskilling", "digital credentials", "learning experience platform", "skills-based organization", "skills-first hiring", "manufacturing workforce", "factory training"
- **å¿…é ˆ**: æ¤œç´¢çµæœã®URLã«å¯¾ã—ã¦**web_fetch**ãƒ„ãƒ¼ãƒ«ã‚’å¿…ãšä½¿ç”¨ã—ã€è¨˜äº‹æœ¬æ–‡ã‚’èª­ã‚“ã§æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

# ğŸ“„ å‡ºåŠ›æƒ…å ±ã‚¹ã‚­ãƒ¼ãƒ
åé›†ã—ãŸã™ã¹ã¦ã®è¨˜äº‹ã®æƒ…å ±ã‚’ã€ä»¥ä¸‹ã®JSONã‚¹ã‚­ãƒ¼ãƒã«å¾“ã†**JSONé…åˆ—**ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
æœ€çµ‚çš„ãªå‡ºåŠ›ã¯ã€ã“ã®JSONé…åˆ—ã®ã¿ã¨ã—ã€ä»–ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚„èª¬æ˜ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚

```json
[
  {{
    "title": "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«",
    "url": "æ­£ç¢ºãªè¨˜äº‹ã®URL",
    "source": "æƒ…å ±æºï¼ˆãƒ¡ãƒ‡ã‚£ã‚¢åï¼‰",
    "published_date": "YYYY-MM-DD",
    "region": "å¯¾è±¡åœ°åŸŸ/å›½",
    "category": "feature/partnership/case_study/integration/funding/acquisition/pricing/regulation/research/dev_updateã®ã„ãšã‚Œã‹",
    "related_companies": ["ä¼æ¥­å1", "ä¼æ¥­å2"],
    "summary_japanese": "3ï½5æ–‡ã®æ—¥æœ¬èªè¦ç´„",
    "key_points": ["é‡è¦ãƒã‚¤ãƒ³ãƒˆ1", "é‡è¦ãƒã‚¤ãƒ³ãƒˆ2", "é‡è¦ãƒã‚¤ãƒ³ãƒˆ3"],
    "tags": ["tag1", "tag2", "tag3"],
    "manufacturing_relevance": "ã‚ã‚Š" or "ãªã—",
    "relevance_reason": "é–¢é€£æ€§ãŒã‚ã‚‹å ´åˆã®ç†ç”±ï¼ˆ1ï½2æ–‡ï¼‰",
    "confidence_score": 0.0ã‹ã‚‰1.0ã®é–“ã®æ•°å€¤
  }},
  // ä»–ã®è¨˜äº‹ã‚‚åŒæ§˜ã«ç¶šã‘ã‚‹
]
```

# âš ï¸ é‡è¦ãªèª¿æŸ»ãƒ«ãƒ¼ãƒ«
1.  **å¿…ãšweb_fetchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦è¨˜äº‹æœ¬æ–‡ã‚’èª­ã‚€ã“ã¨**ã€‚
2.  ç›®æ¨™ä»¶æ•°ã¯**10ï½20ä»¶**ã®è¨˜äº‹ã‚’åé›†ã™ã‚‹ã“ã¨ã€‚
3.  æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„è¨˜äº‹ã‚’å„ªå…ˆã™ã‚‹ã“ã¨ï¼ˆç‰¹ã«è£½é€ æ¥­é–¢é€£ï¼‰ã€‚
4.  é‡è¤‡è¨˜äº‹ã¯é™¤å¤–ã™ã‚‹ã“ã¨ã€‚
5.  æœ€çµ‚å‡ºåŠ›ã¯ã€ä¸Šè¨˜ã®ã‚¹ã‚­ãƒ¼ãƒã«å¾“ã†**JSONé…åˆ—ã®ã¿**ã§ã‚ã‚‹ã“ã¨ã€‚
"""
    print("ğŸ” ã‚¹ã‚­ãƒ«ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆãƒ»ã‚¿ãƒ¬ãƒ³ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã®æœ€æ–°å‹•å‘èª¿æŸ»ã‚’é–‹å§‹ã—ã¾ã™...")

    # --- 6. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œ ---
    MAX_RETRIES = 5
    INITIAL_DELAY = 10
    raw_json_output = None

    for attempt in range(MAX_RETRIES):
        try:
            if attempt > 0:
                delay = INITIAL_DELAY * (2 ** (attempt - 1))
                print(f"\nâš ï¸ Quotaè¶…éã®ãŸã‚ã€{delay:.0f}ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¾ã™... (è©¦è¡Œå›æ•°: {attempt}/{MAX_RETRIES})")
                time.sleep(delay)

            response = agent_executor.invoke({"messages": [HumanMessage(content=search_prompt)]})
            
            messages = response.get("messages", [])
            if messages and hasattr(messages[-1], "content"):
                raw_json_output = messages[-1].content
                try:
                    parsed_data = json.loads(raw_json_output)
                    if isinstance(parsed_data, list) and parsed_data:
                        print(f"âœ… JSONãƒ‡ãƒ¼ã‚¿ã‚’æ­£å¸¸ã«å–å¾—ã—ã¾ã—ãŸã€‚è¨˜äº‹æ•°: {len(parsed_data)}ä»¶")
                        break
                    else:
                        raise ValueError("JSONã®å½¢å¼ãŒæœŸå¾…é€šã‚Šï¼ˆéç©ºã®é…åˆ—ï¼‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

                except json.JSONDecodeError:
                    print("\nâŒ æœ€çµ‚å‡ºåŠ›ãŒæœ‰åŠ¹ãªJSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å†è©¦è¡Œã—ã¾ã™ã€‚")
                    if attempt == MAX_RETRIES - 1:
                         raise
                    continue
                
            else:
                print("âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                if attempt == MAX_RETRIES - 1:
                    sys.exit(1)
                continue # å†è©¦è¡Œã¸

        except Exception as e:
            error_message = str(e)
            if "429 You exceeded your current quota" in error_message or "ResourceExhausted" in error_message:
                if attempt == MAX_RETRIES - 1:
                    print(f"\nâŒ æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸã€‚APIã®å‰²ã‚Šå½“ã¦ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    traceback.print_exc()
                    sys.exit(1)
                continue
            
            print(f"\nâŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_message}")
            traceback.print_exc()
            sys.exit(1)

    # --- 7. JSONãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ ---
    os.makedirs("reports", exist_ok=True)
    
    try:
        with open(RESEARCH_DATA_PATH, "w", encoding="utf-8") as f:
            f.write(json.dumps(parsed_data, indent=2, ensure_ascii=False))

        print("\n" + "=" * 60)
        print("âœ“ ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
        print(f"ğŸ’¾ ä¿å­˜å…ˆ: {RESEARCH_DATA_PATH}")
        print("=" * 60 + "\n")
        
        return RESEARCH_DATA_PATH

    except Exception as e:
        print(f"\nâŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    year_arg = None
    if len(sys.argv) > 1:
        try:
            year_arg = int(sys.argv[1])
        except ValueError:
            print("âš ï¸ å¹´æŒ‡å®šãŒä¸æ­£ã§ã™ã€‚æ•´æ•°ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ä¾‹: python research_searcher.py 2026")

    search_and_extract_data(target_year=year_arg)
