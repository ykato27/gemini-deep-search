"""
ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

éå»ã®é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ï¼ˆreports/weekly_data/*.jsonï¼‰ã‚’é›†è¨ˆã—ã€
æ™‚ç³»åˆ—ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«:
- reports/trends/keywords.json: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾é »åº¦ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
- reports/trends/companies.json: ä¼æ¥­å‡ºç¾é »åº¦ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
- reports/trends/categories.json: ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
- reports/trends/summary.json: é€±æ¬¡ã‚µãƒãƒªãƒ¼ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
"""

import json
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path


def load_weekly_data():
    """é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦èª­ã¿è¾¼ã‚€"""
    weekly_data_dir = Path("reports/weekly_data")

    if not weekly_data_dir.exists():
        print("âš ï¸ reports/weekly_data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []

    weekly_files = sorted(weekly_data_dir.glob("*.json"))

    if not weekly_files:
        print("âš ï¸ é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []

    data = []
    for file in weekly_files:
        try:
            with open(file, "r", encoding="utf-8") as f:
                data.append(json.load(f))
        except Exception as e:
            print(f"âš ï¸ {file.name} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    return data


def analyze_keyword_trends(weekly_data_list):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ"""
    keyword_trends = defaultdict(list)

    for data in weekly_data_list:
        report_date = data["metadata"]["report_date"]
        articles = data.get("articles", [])

        # å„é€±ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        keyword_counts = Counter()
        for article in articles:
            tags = article.get("tags", [])
            for tag in tags:
                keyword_counts[tag] += 1

        # å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã¤ã„ã¦ã€ãã®é€±ã®å‡ºç¾å›æ•°ã‚’è¨˜éŒ²
        for keyword, count in keyword_counts.items():
            keyword_trends[keyword].append({
                "date": report_date,
                "count": count
            })

    # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§å®Œå…¨ã«ã™ã‚‹ï¼ˆ0åŸ‹ã‚ï¼‰
    all_dates = sorted(set(
        data["metadata"]["report_date"] for data in weekly_data_list
    ))

    complete_trends = {}
    for keyword, trend_data in keyword_trends.items():
        date_counts = {item["date"]: item["count"] for item in trend_data}
        complete_trends[keyword] = [
            {"date": date, "count": date_counts.get(date, 0)}
            for date in all_dates
        ]

    # ç·å‡ºç¾å›æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆãƒˆãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆï¼‰
    sorted_keywords = sorted(
        complete_trends.items(),
        key=lambda x: sum(item["count"] for item in x[1]),
        reverse=True
    )

    return dict(sorted_keywords)


def analyze_company_trends(weekly_data_list):
    """ä¼æ¥­ã®æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ"""
    company_trends = defaultdict(list)

    for data in weekly_data_list:
        report_date = data["metadata"]["report_date"]
        articles = data.get("articles", [])

        # å„é€±ã®ä¼æ¥­å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        company_counts = Counter()
        for article in articles:
            companies = article.get("related_companies", [])
            for company in companies:
                company_counts[company] += 1

        # å…¨ä¼æ¥­ã«ã¤ã„ã¦ã€ãã®é€±ã®å‡ºç¾å›æ•°ã‚’è¨˜éŒ²
        for company, count in company_counts.items():
            company_trends[company].append({
                "date": report_date,
                "count": count
            })

    # å„ä¼æ¥­ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§å®Œå…¨ã«ã™ã‚‹ï¼ˆ0åŸ‹ã‚ï¼‰
    all_dates = sorted(set(
        data["metadata"]["report_date"] for data in weekly_data_list
    ))

    complete_trends = {}
    for company, trend_data in company_trends.items():
        date_counts = {item["date"]: item["count"] for item in trend_data}
        complete_trends[company] = [
            {"date": date, "count": date_counts.get(date, 0)}
            for date in all_dates
        ]

    # ç·å‡ºç¾å›æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆãƒˆãƒƒãƒ—ä¼æ¥­ã‚’å„ªå…ˆï¼‰
    sorted_companies = sorted(
        complete_trends.items(),
        key=lambda x: sum(item["count"] for item in x[1]),
        reverse=True
    )

    return dict(sorted_companies)


def analyze_category_trends(weekly_data_list):
    """ã‚«ãƒ†ã‚´ãƒªã®æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ"""
    category_trends = defaultdict(list)

    for data in weekly_data_list:
        report_date = data["metadata"]["report_date"]
        category_dist = data.get("extracted_insights", {}).get("category_distribution", {})

        for category, count in category_dist.items():
            category_trends[category].append({
                "date": report_date,
                "count": count
            })

    # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§å®Œå…¨ã«ã™ã‚‹ï¼ˆ0åŸ‹ã‚ï¼‰
    all_dates = sorted(set(
        data["metadata"]["report_date"] for data in weekly_data_list
    ))

    complete_trends = {}
    for category, trend_data in category_trends.items():
        date_counts = {item["date"]: item["count"] for item in trend_data}
        complete_trends[category] = [
            {"date": date, "count": date_counts.get(date, 0)}
            for date in all_dates
        ]

    return complete_trends


def generate_summary(weekly_data_list):
    """é€±æ¬¡ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
    summary = []

    for data in weekly_data_list:
        metadata = data["metadata"]
        insights = data.get("extracted_insights", {})

        summary.append({
            "report_date": metadata["report_date"],
            "start_date": metadata.get("start_date", ""),
            "end_date": metadata.get("end_date", ""),
            "article_count": metadata.get("article_count", 0),
            "manufacturing_related_count": insights.get("manufacturing_related_count", 0),
            "avg_confidence_score": insights.get("avg_confidence_score", 0.0),
            "top_keywords": insights.get("top_keywords", [])[:5],
            "top_companies": insights.get("top_companies", [])[:3],
            "execution_time": metadata.get("execution_time", "")
        })

    return sorted(summary, key=lambda x: x["report_date"])


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("=" * 60)

    # é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    weekly_data_list = load_weekly_data()

    if not weekly_data_list:
        print("âŒ åˆ†æã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"âœ“ {len(weekly_data_list)}ä»¶ã®é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

    # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    trends_dir = Path("reports/trends")
    trends_dir.mkdir(parents=True, exist_ok=True)

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¬ãƒ³ãƒ‰ã®åˆ†æ
    print("\nğŸ“ˆ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æä¸­...")
    keyword_trends = analyze_keyword_trends(weekly_data_list)
    with open(trends_dir / "keywords.json", "w", encoding="utf-8") as f:
        json.dump(keyword_trends, f, ensure_ascii=False, indent=2)
    print(f"âœ“ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {len(keyword_trends)}ç¨®é¡")

    # ä¼æ¥­ãƒˆãƒ¬ãƒ³ãƒ‰ã®åˆ†æ
    print("\nğŸ¢ ä¼æ¥­ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æä¸­...")
    company_trends = analyze_company_trends(weekly_data_list)
    with open(trends_dir / "companies.json", "w", encoding="utf-8") as f:
        json.dump(company_trends, f, ensure_ascii=False, indent=2)
    print(f"âœ“ ä¼æ¥­: {len(company_trends)}ç¤¾")

    # ã‚«ãƒ†ã‚´ãƒªãƒˆãƒ¬ãƒ³ãƒ‰ã®åˆ†æ
    print("\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æä¸­...")
    category_trends = analyze_category_trends(weekly_data_list)
    with open(trends_dir / "categories.json", "w", encoding="utf-8") as f:
        json.dump(category_trends, f, ensure_ascii=False, indent=2)
    print(f"âœ“ ã‚«ãƒ†ã‚´ãƒª: {len(category_trends)}ç¨®é¡")

    # é€±æ¬¡ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
    print("\nğŸ“‹ é€±æ¬¡ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆä¸­...")
    summary = generate_summary(weekly_data_list)
    with open(trends_dir / "summary.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"âœ“ ã‚µãƒãƒªãƒ¼: {len(summary)}é€±åˆ†")

    # çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
    print("\n" + "=" * 60)
    print("âœ“ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå®Œäº†")
    print("=" * 60)
    print(f"ğŸ“ ä¿å­˜å…ˆ: {trends_dir}/")
    print(f"ğŸ“Š åˆ†ææœŸé–“: {summary[0]['report_date']} ï½ {summary[-1]['report_date']}")
    print(f"ğŸ“° ç·è¨˜äº‹æ•°: {sum(s['article_count'] for s in summary)}ä»¶")
    print(f"ğŸ­ è£½é€ æ¥­é–¢é€£: {sum(s['manufacturing_related_count'] for s in summary)}ä»¶")

    # ãƒˆãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå…¨æœŸé–“ï¼‰
    top_keywords = sorted(
        keyword_trends.items(),
        key=lambda x: sum(item["count"] for item in x[1]),
        reverse=True
    )[:5]
    print(f"\nğŸ·ï¸ ãƒˆãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå…¨æœŸé–“ï¼‰:")
    for keyword, trend_data in top_keywords:
        total = sum(item["count"] for item in trend_data)
        print(f"  â€¢ {keyword}: {total}å›")

    # ãƒˆãƒƒãƒ—ä¼æ¥­ï¼ˆå…¨æœŸé–“ï¼‰
    if company_trends:
        top_companies = sorted(
            company_trends.items(),
            key=lambda x: sum(item["count"] for item in x[1]),
            reverse=True
        )[:5]
        print(f"\nğŸ¢ ãƒˆãƒƒãƒ—ä¼æ¥­ï¼ˆå…¨æœŸé–“ï¼‰:")
        for company, trend_data in top_companies:
            total = sum(item["count"] for item in trend_data)
            print(f"  â€¢ {company}: {total}å›")

    print("=" * 60 + "\n")


if __name__ == "__main__":
    main()
