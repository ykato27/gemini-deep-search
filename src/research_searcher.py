"""
Phase 1: äº‹ä¾‹æ¤œç´¢ãƒ»ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æœ€é©åŒ–ç‰ˆï¼‰
- Phase 1: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§æ§‹é€ åŒ–ã•ã‚Œã¦ã„ãªã„ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º
- Phase 2: åˆ¥ã®LLMã‚³ãƒ¼ãƒ«ã§ã‚·ãƒ³ãƒ—ãƒ«ã«JSONæ•´å½¢ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å¤§å¹…å‰Šæ¸›ï¼‰
- ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–: é©åˆ‡ãªå¾…æ©Ÿæ™‚é–“ã¨ãƒãƒƒãƒå‡¦ç†
"""
import os
import sys
import time
import json
import traceback
import warnings
import re
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime

# Suppress LangGraph deprecation warnings
warnings.filterwarnings("ignore", category=DeprecationWarning, module="langgraph")
warnings.filterwarnings("ignore", message=".*create_react_agent.*")

from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_tavily import TavilySearch
# Note: create_react_agent shows deprecation warning but still works in current version
from langgraph.prebuilt import create_react_agent

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
from config_loader import get_config


def parse_publication_date(date_str: str):
    if not date_str:
        return None

    normalized = date_str.strip()
    lowered = normalized.lower()
    if lowered in {"", "n/a", "na", "unknown"}:
        return None

    if normalized in {"ä¸æ˜", "æœªè¨­å®š", "ä¸è©³", "â€•"}:
        return None

    if '\ufffd' in normalized:
        return None

    # Handle relative expressions such as "3 days ago" or "last week"
    relative_match = re.match(r"^(\d{1,2})\s+(day|days|hour|hours|week|weeks)\s+ago$", lowered)
    if relative_match:
        value, unit = relative_match.groups()
        amount = int(value)
        now = datetime.now()
        if unit.startswith("day"):
            return now - timedelta(days=amount)
        if unit.startswith("hour"):
            return now - timedelta(hours=amount)
        if unit.startswith("week"):
            return now - timedelta(weeks=amount)

    if lowered in {"yesterday", "æ˜¨æ—¥"}:
        return datetime.now() - timedelta(days=1)
    if lowered in {"today", "æœ¬æ—¥", "ãã‚‡ã†", "ä»Šæ—¥"}:
        return datetime.now()

    # Handle Japanese date expressions such as "2024å¹´5æœˆ20æ—¥"
    jp_date_match = re.match(r"^(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥$", normalized)
    if jp_date_match:
        year, month, day = map(int, jp_date_match.groups())
        return datetime(year, month, day)

    # Handle compact numeric formats such as 20240520
    if re.fullmatch(r"\d{8}", normalized):
        try:
            return datetime.strptime(normalized, "%Y%m%d")
        except ValueError:
            pass

    date_formats = [
        "%Y-%m-%d",
        "%Y/%m/%d",
        "%Y.%m.%d",
        "%d %B %Y",
        "%d %b %Y",
        "%B %d, %Y",
        "%b %d, %Y",
        "%B %d %Y",
        "%b %d %Y",
        "%m/%d/%Y",
        "%m-%d-%Y",
        "%d/%m/%Y",
        "%d-%m-%Y",
        "%Y-%m",
        "%Y/%m",
        "%Y.%m",
        "%Y",
    ]

    # Remove ordinal suffixes from English dates (e.g., "May 5th, 2024")
    normalized_no_suffix = re.sub(r"(\d+)(st|nd|rd|th)", r"\1", normalized, flags=re.IGNORECASE)

    for fmt in date_formats:
        try:
            parsed = datetime.strptime(normalized_no_suffix, fmt)
            if fmt in {"%Y-%m", "%Y/%m", "%Y.%m"}:
                return parsed.replace(day=1)
            if fmt == "%Y":
                return parsed.replace(month=1, day=1)
            return parsed
        except ValueError:
            continue

    # Try ISO 8601 style formats (with or without timezone)
    iso_candidate = normalized
    if iso_candidate.endswith("Z"):
        iso_candidate = iso_candidate[:-1] + "+00:00"

    try:
        parsed = datetime.fromisoformat(iso_candidate)
        if parsed.tzinfo is not None:
            parsed = parsed.astimezone(timezone.utc)
        return parsed
    except ValueError:
        pass

    # Fallback to RFC 2822 and other email style date strings
    try:
        parsed = parsedate_to_datetime(normalized_no_suffix)
        if parsed.tzinfo is not None:
            parsed = parsed.astimezone(timezone.utc)
        return parsed
    except (TypeError, ValueError, OverflowError):
        pass

    return None



def search_and_extract_data(target_year: int = None):
    """
    é€±æ¬¡èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã‚’Webæ¤œç´¢ã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸJSONã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
    Phase 1: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§éæ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    Phase 2: åˆ¥LLMã‚³ãƒ¼ãƒ«ã§JSONæ•´å½¢ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ï¼‰
    """
    print("\n" + "=" * 60)
    print("ğŸš€ Phase 1: äº‹ä¾‹æ¤œç´¢ã¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’é–‹å§‹")
    print("=" * 60)

    # --- 0. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ---
    config = get_config()

    # --- 1. ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª ---
    google_api_key = os.environ.get("GOOGLE_API_KEY")
    tavily_api_key = os.environ.get("TAVILY_API_KEY")

    if not google_api_key or not tavily_api_key:
        print("âŒ ã‚¨ãƒ©ãƒ¼: GOOGLE_API_KEY/TAVILY_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)
    print("âœ“ APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¾ã—ãŸ")

    # --- 2. æ¤œç´¢å¯¾è±¡å¹´ã®è¨­å®šã¨æœŸé–“ã®è¨ˆç®— ---
    today = datetime.now()
    days_back = config.get("search.days_back", 7)
    start_date = (today - timedelta(days=days_back)).strftime("%Y-%m-%d")
    end_date = today.strftime("%Y-%m-%d")
    year = target_year or today.year

    print(f"ğŸ“… æ¤œç´¢å¯¾è±¡å¹´: {year}")
    print(f"ğŸ—“ï¸ æ¤œç´¢é–‹å§‹æ—¥: {start_date} (éå»{days_back}æ—¥é–“)")

    # --- 3. LLMã¨ãƒ„ãƒ¼ãƒ«ã®æº–å‚™ ---
    model = ChatGoogleGenerativeAI(
        model=config.get("llm.searcher.model", "gemini-2.5-flash"),
        temperature=config.get("llm.searcher.temperature", 0),
    )

    search_tool = TavilySearch(
        max_results=config.get("tavily.max_results", 5),
        search_depth=config.get("tavily.search_depth", "advanced"),
        include_raw_content=config.get("tavily.include_raw_content", False),
        start_date=start_date,
    )
    tools = [search_tool]

    # --- 4. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ ---
    agent_executor = create_react_agent(model, tools)
    print("âœ“ ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®šã—ã¾ã—ãŸ")

    # --- 5. Phase 1: éæ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç°¡æ½”ç‰ˆï¼‰ ---
    min_articles = config.get("search.min_articles", 3)
    max_articles = config.get("search.max_articles", 5)
    keywords = config.get("search.keywords", [
        "skills management latest trends",
        "talent management workforce news"
    ])
    keywords_str = "\n   - ".join([f'"{kw}"' for kw in keywords])

    search_prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªãƒªã‚µãƒ¼ãƒã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’**åŠ¹ç‡çš„ã«**å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

# ã‚¿ã‚¹ã‚¯
éå»{days_back}æ—¥é–“ï¼ˆ{start_date}ä»¥é™ï¼‰ã®**è£½é€ æ¥­å‘ã‘ã‚¹ã‚­ãƒ«ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆãƒ»ã‚¿ãƒ¬ãƒ³ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ**é–¢é€£ã®æ¬§ç±³è¨˜äº‹ã‚’**{min_articles}ï½{max_articles}ä»¶**åé›†ã—ã€ç°¡æ½”ã«æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

# æ¤œç´¢æ–¹æ³•
1. ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‹ã‚‰ã€**æœ€ã‚‚åŠ¹æœçš„ã¨æ€ã‚ã‚Œã‚‹3ï½5å€‹ã‚’é¸ã‚“ã§æ¤œç´¢**ã—ã¦ãã ã•ã„ï¼š
   {keywords_str}

2. æ¤œç´¢ã®å„ªå…ˆé †ä½ï¼š
   - **è£½é€ æ¥­ï¼ˆmanufacturing, industrial, plant, factoryï¼‰ã«é–¢é€£ã™ã‚‹è¨˜äº‹ã‚’å„ªå…ˆ**
   - å…·ä½“çš„ãªä¼æ¥­åãƒ»ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆåï¼ˆAG5, Kahuna, Skills Base, iMocha, Indeavorç­‰ï¼‰ãŒå«ã¾ã‚Œã‚‹è¨˜äº‹
   - Industry 4.0ã€ã‚¹ãƒãƒ¼ãƒˆãƒãƒ‹ãƒ¥ãƒ•ã‚¡ã‚¯ãƒãƒ£ãƒªãƒ³ã‚°ã€ã‚¹ã‚­ãƒ«ã‚®ãƒ£ãƒƒãƒ—åˆ†æã«é–¢ã™ã‚‹è¨˜äº‹
   - å®Ÿè·µçš„ãªã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã‚„å°å…¥äº‹ä¾‹

3. æ¤œç´¢çµæœã‹ã‚‰**æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„{min_articles}ï½{max_articles}è¨˜äº‹**ã‚’é¸ã‚“ã§ãã ã•ã„

4. **web_fetchãƒ„ãƒ¼ãƒ«ã¯ä½¿ç”¨ã›ãš**ã€æ¤œç´¢çµæœã®ã‚¹ãƒ‹ãƒšãƒƒãƒˆæƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ã®ãŸã‚ï¼‰

# å‡ºåŠ›å½¢å¼
å„è¨˜äº‹ã‚’ä»¥ä¸‹ã®**ç°¡æ½”ãªå½¢å¼**ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

---
è¨˜äº‹ 1
ã‚¿ã‚¤ãƒˆãƒ«: [ã‚¿ã‚¤ãƒˆãƒ«]
URL: [URL]
æƒ…å ±æº: [ãƒ¡ãƒ‡ã‚£ã‚¢å]
å…¬é–‹æ—¥: [YYYY-MM-DDå½¢å¼ã§è¨˜è¼‰ã€‚ä¸æ˜ãªå ´åˆã¯ã€Œä¸æ˜ã€]
åœ°åŸŸ: [å›½/åœ°åŸŸ]
ã‚«ãƒ†ã‚´ãƒªãƒ¼: [feature/case_study/partnership/etc]
é–¢é€£ä¼æ¥­: [ä¼æ¥­åã€ãªã‘ã‚Œã°ã€Œãªã—ã€]
è¦ç´„: [2ï½3æ–‡ã®æ—¥æœ¬èªè¦ç´„]
é‡è¦ãƒã‚¤ãƒ³ãƒˆ: [ãƒã‚¤ãƒ³ãƒˆ1] / [ãƒã‚¤ãƒ³ãƒˆ2] / [ãƒã‚¤ãƒ³ãƒˆ3]
ã‚¿ã‚°: [tag1, tag2, tag3]
è£½é€ æ¥­é–¢é€£: [ã‚ã‚Š/ãªã—]
é–¢é€£æ€§ç†ç”±: [1æ–‡ã€ãªã‘ã‚Œã°ã€Œè©²å½“ãªã—ã€]
ä¿¡é ¼åº¦: [0.0ï½1.0]
---

# é‡è¦ãªåˆ¶ç´„
- **å…¬é–‹æ—¥ãŒ{start_date}ä»¥é™ï¼ˆéå»{days_back}æ—¥ä»¥å†…ï¼‰ã®è¨˜äº‹ã®ã¿ã‚’é¸æŠã—ã¦ãã ã•ã„**
- å¤ã„è¨˜äº‹ï¼ˆä¾‹ï¼šã€Œ2025å¹´ã®äºˆæ¸¬ã€ã‚’æ‰±ã£ãŸæ•°ãƒ¶æœˆå‰ã®è¨˜äº‹ï¼‰ã¯é™¤å¤–ã—ã¦ãã ã•ã„
- **è£½é€ æ¥­ãƒ»å·¥å ´ãƒ»ãƒ—ãƒ©ãƒ³ãƒˆé–¢é€£ã®è¨˜äº‹ã‚’å„ªå…ˆçš„ã«é¸æŠã—ã¦ãã ã•ã„**
- æ¤œç´¢ã¯**åŠ¹ç‡çš„ã«**ï¼ˆ3ï½5å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§3ï½5å›ç¨‹åº¦ï¼‰å®Ÿæ–½ã—ã¦ãã ã•ã„
- web_fetchã¯**ä½¿ç”¨ã—ãªã„**ã§ãã ã•ã„
- è¨˜äº‹æ•°ã¯**{min_articles}ï½{max_articles}ä»¶**ã§ååˆ†ã§ã™
- ç°¡æ½”ã«æƒ…å ±ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„
"""

    print("ğŸ” æœ€æ–°å‹•å‘èª¿æŸ»ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ãƒ¢ãƒ¼ãƒ‰ï¼‰...")

    # --- 6. Phase 1: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼‰ ---
    MAX_RETRIES = config.get("agent.max_retries", 3)
    INITIAL_DELAY = config.get("agent.initial_delay", 60)
    recursion_limit = config.get("agent.recursion_limit", 30)
    raw_text_output = None

    for attempt in range(MAX_RETRIES):
        try:
            if attempt > 0:
                # APIã‚¯ã‚©ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆå¾…ã¡ï¼ˆ1åˆ†é–“éš”ã‚’è€ƒæ…®ï¼‰
                delay = max(60, INITIAL_DELAY * (2 ** (attempt - 1)))
                print(f"\nâš ï¸ APIã‚¯ã‚©ãƒ¼ã‚¿è¶…éã®ãŸã‚ã€{delay:.0f}ç§’å¾…æ©Ÿã—ã¾ã™... (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES})")
                time.sleep(delay)

            print(f"ğŸ“¡ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œä¸­... (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES})")
            response = agent_executor.invoke(
                {"messages": [HumanMessage(content=search_prompt)]},
                config={"recursion_limit": recursion_limit}
            )
            
            messages = response.get("messages", [])
            if messages and hasattr(messages[-1], "content"):
                content = messages[-1].content

                # contentãŒãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆï¼ˆæ–°ã—ã„APIå½¢å¼ï¼‰ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                if isinstance(content, list) and len(content) > 0:
                    # ãƒªã‚¹ãƒˆã®æœ€åˆã®è¦ç´ ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                    if isinstance(content[0], dict) and 'text' in content[0]:
                        raw_text_output = content[0]['text']
                    else:
                        raw_text_output = str(content)
                else:
                    # å¾“æ¥ã®æ–‡å­—åˆ—å½¢å¼
                    raw_text_output = content

                # ãƒ‡ãƒãƒƒã‚°: å®Ÿéš›ã®å‡ºåŠ›å†…å®¹ã‚’è¡¨ç¤º
                preview_length = config.get("debug.preview_length", 500)
                if config.get("debug.enabled", False):
                    print(f"\nğŸ“Š ãƒ‡ãƒãƒƒã‚°: å‡ºåŠ›æ–‡å­—æ•° = {len(raw_text_output)}")
                    print(f"ğŸ“Š ãƒ‡ãƒãƒƒã‚°: å‡ºåŠ›ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€åˆã®{preview_length}æ–‡å­—ï¼‰:\n{raw_text_output[:preview_length]}\n")

                # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã®ç°¡æ˜“æ¤œè¨¼ï¼ˆæœ€ä½800æ–‡å­—ã‚’è¦æ±‚ã—ã€è¨˜äº‹æƒ…å ±ã®å­˜åœ¨ã‚’ç¢ºèªï¼‰
                min_chars = 800
                has_article_markers = "è¨˜äº‹" in raw_text_output or "ã‚¿ã‚¤ãƒˆãƒ«" in raw_text_output
                has_enough_content = len(raw_text_output) > min_chars

                if has_enough_content and has_article_markers:
                    print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚æ–‡å­—æ•°: {len(raw_text_output)}")
                    break
                else:
                    print("\nâš ï¸ å‡ºåŠ›ãŒä¸ååˆ†ã§ã™ã€‚å†è©¦è¡Œã—ã¾ã™ã€‚")
                    print(f"   - æ–‡å­—æ•°æ¡ä»¶: {has_enough_content} (å®Ÿéš›: {len(raw_text_output)}æ–‡å­—ã€æœ€ä½: {min_chars}æ–‡å­—)")
                    print(f"   - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¡ä»¶: {has_article_markers}")
                    if attempt == MAX_RETRIES - 1:
                        # æœ€å¾Œã®è©¦è¡Œã§ã‚‚å¤±æ•—ã—ãŸå ´åˆã€éƒ¨åˆ†çš„ãªçµæœã§ã‚‚ä½¿ç”¨
                        if raw_text_output and len(raw_text_output) > 200:
                            print("âš ï¸ éƒ¨åˆ†çš„ãªçµæœã‚’ä½¿ç”¨ã—ã¾ã™")
                            print(f"\nğŸ“Š å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰:\n{raw_text_output[:500]}\n")
                            break
                        print(f"\nğŸ“Š æœ€çµ‚çš„ãªå‡ºåŠ›å†…å®¹ï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰:\n{raw_text_output[:1000]}\n")
                        raise ValueError("æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                    continue
            else:
                print("âŒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã®å‡ºåŠ›å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                print(f"ğŸ“Š ãƒ‡ãƒãƒƒã‚°: messages = {messages}")
                if attempt == MAX_RETRIES - 1:
                    sys.exit(1)
                continue

        except Exception as e:
            error_message = str(e)
            if "429" in error_message or "ResourceExhausted" in error_message or "Quota exceeded" in error_message:
                if attempt == MAX_RETRIES - 1:
                    print(f"\nâŒ æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸã€‚")
                    print("ğŸ’¡ å¯¾ç­–: ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†å®Ÿè¡Œã™ã‚‹ã‹ã€æœ‰æ–™ãƒ—ãƒ©ãƒ³ã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
                    print("ğŸ“Š Gemini APIç„¡æ–™æ : 1åˆ†ã‚ãŸã‚Š250,000ãƒˆãƒ¼ã‚¯ãƒ³")
                    traceback.print_exc()
                    sys.exit(1)
                print(f"â³ APIã‚¯ã‚©ãƒ¼ã‚¿è¶…éã‚’æ¤œå‡ºã€‚å¾…æ©Ÿå¾Œã«å†è©¦è¡Œã—ã¾ã™...")
                continue
            
            print(f"\nâŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_message}")
            traceback.print_exc()
            if attempt == MAX_RETRIES - 1:
                sys.exit(1)
            continue

    if not raw_text_output:
        print("âŒ ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)

    # --- 7. Phase 2: JSONæ•´å½¢ï¼ˆåˆ¥LLMã‚³ãƒ¼ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ï¼‰ ---
    print("\n" + "=" * 60)
    print("ğŸ”„ Phase 2: JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¸ã®å¤‰æ›ã‚’é–‹å§‹")
    print("=" * 60)
    
    # ã‚¯ã‚©ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆå¾…ã¡
    print("â³ APIã‚¯ã‚©ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆã®ãŸã‚60ç§’å¾…æ©Ÿã—ã¾ã™...")
    time.sleep(60)
    
    # JSONæ•´å½¢ç”¨ã®è»½é‡LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå±¥æ­´ãªã—ï¼‰
    formatting_model = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
    )
    
    json_formatting_prompt = f"""
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã«ã¯è¨˜äº‹æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚’JSONé…åˆ—ã«æ•´å½¢ã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ:
{raw_text_output}

å‡ºåŠ›: ä»¥ä¸‹ã®å½¢å¼ã®JSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ï¼ˆèª¬æ˜æ–‡ã‚„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯è¨˜å·ãªã—ï¼‰

[
  {{
    "title": "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«",
    "url": "URL",
    "source": "æƒ…å ±æº",
    "published_date": "YYYY-MM-DD",
    "region": "åœ°åŸŸ",
    "category": "ã‚«ãƒ†ã‚´ãƒªãƒ¼",
    "related_companies": ["ä¼æ¥­å"],
    "summary_japanese": "è¦ç´„",
    "key_points": ["ãƒã‚¤ãƒ³ãƒˆ1", "ãƒã‚¤ãƒ³ãƒˆ2", "ãƒã‚¤ãƒ³ãƒˆ3"],
    "tags": ["tag1", "tag2"],
    "manufacturing_relevance": "ã‚ã‚Š or ãªã—",
    "relevance_reason": "ç†ç”± or è©²å½“ãªã—",
    "confidence_score": 0.0ï½1.0ã®æ•°å€¤
  }}
]

é‡è¦: JSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã€‚å‰å¾Œã«ä¸€åˆ‡ã®èª¬æ˜ã‚„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’å«ã‚ãªã„ã“ã¨ã€‚
"""

    for attempt in range(MAX_RETRIES):
        try:
            if attempt > 0:
                delay = 60
                print(f"\nâš ï¸ å¾…æ©Ÿä¸­... (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES})")
                time.sleep(delay)

            print(f"ğŸ”„ JSONå¤‰æ›ä¸­... (è©¦è¡Œ {attempt + 1}/{MAX_RETRIES})")
            formatting_response = formatting_model.invoke([HumanMessage(content=json_formatting_prompt)])
            json_output = formatting_response.content
            
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰Šé™¤
            json_output = json_output.strip()
            if json_output.startswith("```json"):
                json_output = json_output[7:]
            if json_output.startswith("```"):
                json_output = json_output[3:]
            if json_output.endswith("```"):
                json_output = json_output[:-3]
            json_output = json_output.strip()
            
            # JSONãƒ‘ãƒ¼ã‚¹ã‚’è©¦è¡Œ
            parsed_data = json.loads(json_output)

            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®å‡ºåŠ›
            if config.get("debug.enabled", False) or attempt > 0:
                print(f"\nğŸ“Š ãƒ‡ãƒãƒƒã‚°: JSONå‹ = {type(parsed_data)}")
                if isinstance(parsed_data, list):
                    print(f"ğŸ“Š ãƒ‡ãƒãƒƒã‚°: é…åˆ—ã®é•·ã• = {len(parsed_data)}")
                print(f"ğŸ“Š ãƒ‡ãƒãƒƒã‚°: JSONå‡ºåŠ›ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰:\n{json_output[:500]}\n")

            if isinstance(parsed_data, list) and len(parsed_data) > 0:
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: start_dateä»¥é™ã®è¨˜äº‹ã®ã¿ã‚’ä¿æŒ
                start_date_limit = datetime.strptime(start_date, "%Y-%m-%d").date()
                end_date_limit = datetime.strptime(end_date, "%Y-%m-%d").date()

                original_count = len(parsed_data)
                filtered_data = []
                for article in parsed_data:
                    pub_date_str = article.get("published_date")

                    # Filter articles to the requested 7-day window
                    parsed_datetime = parse_publication_date(pub_date_str)

                    if not parsed_datetime:
                        print(
                            f"[WARN] Skipping article with unparsed date: {article.get('title', 'Unknown title')}"
                            f" (published_date={pub_date_str})"
                        )
                        continue

                    published_date = parsed_datetime.date()

                    if published_date < start_date_limit or published_date > end_date_limit:
                        print(f"[WARN] Skipping article outside window: {article.get('title', 'Unknown title')} (published_date={pub_date_str})")
                        continue

                    filtered_data.append(article)
                parsed_data = filtered_data

                if len(parsed_data) > 0:
                    print(f"âœ… JSONãƒ‡ãƒ¼ã‚¿ã‚’æ­£å¸¸ã«å¤‰æ›ã—ã¾ã—ãŸã€‚è¨˜äº‹æ•°: {len(parsed_data)}ä»¶ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰")
                    if original_count != len(parsed_data):
                        print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ã®è¨˜äº‹æ•°: {original_count}ä»¶")
                    break
                else:
                    print("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®è¨˜äº‹ãŒ0ä»¶ã§ã™ã€‚å†è©¦è¡Œã—ã¾ã™ã€‚")
                    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ã®è¨˜äº‹æ•°: {original_count}ä»¶")
                    print(f"ğŸ“Š æ¤œç´¢æœŸé–“: {start_date} ï½ {end_date}")
                    if attempt == MAX_RETRIES - 1:
                        print(f"\nğŸ“Š å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰:\n{raw_text_output[:500]}\n")
                        raise ValueError("æœ‰åŠ¹ãªè¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ0ä»¶ï¼‰ã€‚æ¤œç´¢æœŸé–“å†…ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    continue
            else:
                # JSONãŒé…åˆ—ã§ãªã„ã€ã¾ãŸã¯ç©ºé…åˆ—ã®å ´åˆ
                error_msg = f"JSONã®å½¢å¼ãŒæœŸå¾…é€šã‚Šã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‹: {type(parsed_data)}"
                if isinstance(parsed_data, list):
                    error_msg = "JSONã¯é…åˆ—ã§ã™ãŒã€ç©ºã§ã™ï¼ˆé•·ã•0ï¼‰ã€‚"
                print(f"âš ï¸ {error_msg}")
                print(f"ğŸ“Š JSONå‡ºåŠ›ï¼ˆæœ€åˆã®1000æ–‡å­—ï¼‰:\n{json_output[:1000]}\n")
                print(f"ğŸ“Š å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰:\n{raw_text_output[:500]}\n")

                if attempt == MAX_RETRIES - 1:
                    raise ValueError(f"{error_msg} å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒä¸ååˆ†ã‹ã€JSONå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                continue

        except json.JSONDecodeError as e:
            print(f"\nâŒ JSONå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            if attempt == MAX_RETRIES - 1:
                print("\nç”Ÿã®JSONå‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰:")
                print(json_output[:2000] if len(json_output) > 2000 else json_output)
                sys.exit(1)
            continue
            
        except Exception as e:
            error_message = str(e)
            if "429" in error_message or "ResourceExhausted" in error_message or "Quota exceeded" in error_message:
                if attempt == MAX_RETRIES - 1:
                    print(f"\nâŒ æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸã€‚")
                    traceback.print_exc()
                    sys.exit(1)
                continue
            
            print(f"\nâŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_message}")
            traceback.print_exc()
            if attempt == MAX_RETRIES - 1:
                sys.exit(1)
            continue

    # --- 8. JSONãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ ---
    research_data_path = config.get("data.research_data_path", "reports/research_data.json")
    reports_dir = os.path.dirname(research_data_path) or "reports"
    os.makedirs(reports_dir, exist_ok=True)

    try:
        with open(research_data_path, "w", encoding="utf-8") as f:
            json.dump(parsed_data, f, indent=2, ensure_ascii=False)

        print("\n" + "=" * 60)
        print("âœ… ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
        print(f"ğŸ’¾ ä¿å­˜å…ˆ: {research_data_path}")
        print(f"ğŸ“Š è¨˜äº‹æ•°: {len(parsed_data)}ä»¶")
        print("=" * 60 + "\n")

        return research_data_path

    except Exception as e:
        print(f"\nâŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    year_arg = None
    if len(sys.argv) > 1:
        try:
            year_arg = int(sys.argv[1])
        except ValueError:
            print("âš ï¸ å¹´æŒ‡å®šãŒä¸æ­£ã§ã™ã€‚æ•´æ•°ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ä¾‹: python research_searcher.py 2026")

    search_and_extract_data(target_year=year_arg)
