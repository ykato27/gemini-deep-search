"""
Search - è¨˜äº‹æ¤œç´¢ãƒ»è©³ç´°é–²è¦§

éå»ã®è¨˜äº‹ã‚’æ¤œç´¢ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€è©³ç´°é–²è¦§ã™ã‚‹ãƒšãƒ¼ã‚¸ã§ã™ã€‚
"""

import json
from datetime import datetime
from pathlib import Path

import pandas as pd
import streamlit as st

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Search - è¨˜äº‹æ¤œç´¢",
    page_icon="ğŸ”",
    layout="wide"
)

st.title("ğŸ” è¨˜äº‹æ¤œç´¢ãƒ»è©³ç´°é–²è¦§")
st.markdown("éå»ã®è¨˜äº‹ã‚’æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦è©³ç´°ã‚’ç¢ºèªã—ã¾ã™")
st.markdown("---")


@st.cache_data
def load_all_articles():
    """å…¨ã¦ã®é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨˜äº‹ã‚’èª­ã¿è¾¼ã‚€"""
    weekly_data_dir = Path("reports/weekly_data")
    if not weekly_data_dir.exists():
        return []

    all_articles = []
    for file in sorted(weekly_data_dir.glob("*.json")):
        with open(file, "r", encoding="utf-8") as f:
            data = json.load(f)
            report_date = data['metadata']['report_date']

            for article in data.get('articles', []):
                article['report_date'] = report_date
                all_articles.append(article)

    return all_articles


# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
articles = load_all_articles()

if not articles:
    st.error("é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`python weekly_research.py` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ---
st.sidebar.header("ğŸ” æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
search_query = st.sidebar.text_input(
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢",
    placeholder="ã‚¿ã‚¤ãƒˆãƒ«ã€è¦ç´„ã€ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰æ¤œç´¢...",
    help="è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã€è¦ç´„ã€é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’å…¨æ–‡æ¤œç´¢ã—ã¾ã™"
)

# æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
st.sidebar.subheader("ğŸ“… æœŸé–“")
all_dates = sorted(set(article['report_date'] for article in articles))
if len(all_dates) >= 2:
    date_range = st.sidebar.date_input(
        "ãƒ¬ãƒãƒ¼ãƒˆæ—¥ã®ç¯„å›²",
        value=(datetime.strptime(all_dates[0], "%Y-%m-%d").date(),
               datetime.strptime(all_dates[-1], "%Y-%m-%d").date()),
        help="ãƒ¬ãƒãƒ¼ãƒˆæ—¥ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"
    )
else:
    date_range = None
    st.sidebar.info("æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã¯2é€±é–“ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")

# ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
st.sidebar.subheader("ğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒª")
all_categories = sorted(set(
    article.get('category', 'ä¸æ˜')
    for article in articles
    if article.get('category')
))
selected_categories = st.sidebar.multiselect(
    "ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ",
    options=all_categories,
    default=all_categories,
    help="è¤‡æ•°é¸æŠå¯èƒ½"
)

# ä¼æ¥­ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
st.sidebar.subheader("ğŸ¢ ä¼æ¥­")
all_companies = sorted(set(
    company
    for article in articles
    for company in article.get('related_companies', [])
    if company
))

if all_companies:
    selected_companies = st.sidebar.multiselect(
        "ä¼æ¥­ã‚’é¸æŠ",
        options=['(å…¨ã¦)'] + all_companies,
        default=['(å…¨ã¦)'],
        help="ä¼æ¥­ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"
    )
else:
    selected_companies = ['(å…¨ã¦)']
    st.sidebar.info("ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

# è£½é€ æ¥­é–¢é€£ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
st.sidebar.subheader("ğŸ­ è£½é€ æ¥­é–¢é€£")
manufacturing_filter = st.sidebar.radio(
    "è£½é€ æ¥­é–¢é€£ã®ã¿è¡¨ç¤º",
    options=['å…¨ã¦', 'ã‚ã‚Š', 'ãªã—'],
    index=0
)

# ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
st.sidebar.subheader("â­ ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢")
min_confidence = st.sidebar.slider(
    "æœ€å°ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢",
    min_value=0.0,
    max_value=1.0,
    value=0.0,
    step=0.1,
    help="ã“ã®å€¤ä»¥ä¸Šã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’æŒã¤è¨˜äº‹ã®ã¿è¡¨ç¤º"
)

# --- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç† ---
filtered_articles = articles.copy()

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
if search_query:
    search_lower = search_query.lower()
    filtered_articles = [
        article for article in filtered_articles
        if (search_lower in article.get('title', '').lower() or
            search_lower in article.get('summary_japanese', '').lower() or
            any(search_lower in point.lower() for point in article.get('key_points', [])))
    ]

# æ—¥ä»˜ç¯„å›²
if date_range and len(date_range) == 2:
    start_date_str = date_range[0].strftime("%Y-%m-%d")
    end_date_str = date_range[1].strftime("%Y-%m-%d")
    filtered_articles = [
        article for article in filtered_articles
        if start_date_str <= article.get('report_date', '') <= end_date_str
    ]

# ã‚«ãƒ†ã‚´ãƒª
if selected_categories:
    filtered_articles = [
        article for article in filtered_articles
        if article.get('category', 'ä¸æ˜') in selected_categories
    ]

# ä¼æ¥­
if selected_companies and '(å…¨ã¦)' not in selected_companies:
    filtered_articles = [
        article for article in filtered_articles
        if any(company in article.get('related_companies', []) for company in selected_companies)
    ]

# è£½é€ æ¥­é–¢é€£
if manufacturing_filter != 'å…¨ã¦':
    filtered_articles = [
        article for article in filtered_articles
        if article.get('manufacturing_relevance', 'ãªã—') == manufacturing_filter
    ]

# ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
filtered_articles = [
    article for article in filtered_articles
    if article.get('confidence_score', 0) >= min_confidence
]

# --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
st.subheader(f"ğŸ“Š æ¤œç´¢çµæœ: {len(filtered_articles)}ä»¶ / {len(articles)}ä»¶")

# ã‚½ãƒ¼ãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³
col1, col2, col3 = st.columns([2, 1, 1])

with col1:
    sort_by = st.selectbox(
        "ä¸¦ã³æ›¿ãˆ",
        options=['ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆé™é †ï¼‰', 'å…¬é–‹æ—¥ï¼ˆæ–°ã—ã„é †ï¼‰', 'å…¬é–‹æ—¥ï¼ˆå¤ã„é †ï¼‰', 'ãƒ¬ãƒãƒ¼ãƒˆæ—¥ï¼ˆæ–°ã—ã„é †ï¼‰', 'ãƒ¬ãƒãƒ¼ãƒˆæ—¥ï¼ˆå¤ã„é †ï¼‰'],
        index=0
    )

with col2:
    items_per_page = st.selectbox(
        "è¡¨ç¤ºä»¶æ•°",
        options=[10, 25, 50, 100],
        index=1
    )

with col3:
    if st.button("ğŸ“¥ CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"):
        # CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨ã®DataFrameä½œæˆ
        export_data = []
        for article in filtered_articles:
            export_data.append({
                'ã‚¿ã‚¤ãƒˆãƒ«': article.get('title', ''),
                'URL': article.get('url', ''),
                'æƒ…å ±æº': article.get('source', ''),
                'å…¬é–‹æ—¥': article.get('published_date', ''),
                'ãƒ¬ãƒãƒ¼ãƒˆæ—¥': article.get('report_date', ''),
                'åœ°åŸŸ': article.get('region', ''),
                'ã‚«ãƒ†ã‚´ãƒª': article.get('category', ''),
                'é–¢é€£ä¼æ¥­': ', '.join(article.get('related_companies', [])),
                'è£½é€ æ¥­é–¢é€£': article.get('manufacturing_relevance', ''),
                'ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢': article.get('confidence_score', 0),
                'è¦ç´„': article.get('summary_japanese', ''),
                'ã‚¿ã‚°': ', '.join(article.get('tags', []))
            })

        df_export = pd.DataFrame(export_data)
        csv = df_export.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv,
            file_name=f"articles_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )

# ã‚½ãƒ¼ãƒˆå‡¦ç†
if sort_by == 'ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆé™é †ï¼‰':
    filtered_articles = sorted(filtered_articles, key=lambda x: x.get('confidence_score', 0), reverse=True)
elif sort_by == 'å…¬é–‹æ—¥ï¼ˆæ–°ã—ã„é †ï¼‰':
    filtered_articles = sorted(filtered_articles, key=lambda x: x.get('published_date', ''), reverse=True)
elif sort_by == 'å…¬é–‹æ—¥ï¼ˆå¤ã„é †ï¼‰':
    filtered_articles = sorted(filtered_articles, key=lambda x: x.get('published_date', ''))
elif sort_by == 'ãƒ¬ãƒãƒ¼ãƒˆæ—¥ï¼ˆæ–°ã—ã„é †ï¼‰':
    filtered_articles = sorted(filtered_articles, key=lambda x: x.get('report_date', ''), reverse=True)
elif sort_by == 'ãƒ¬ãƒãƒ¼ãƒˆæ—¥ï¼ˆå¤ã„é †ï¼‰':
    filtered_articles = sorted(filtered_articles, key=lambda x: x.get('report_date', ''))

st.markdown("---")

# ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
total_pages = (len(filtered_articles) - 1) // items_per_page + 1 if filtered_articles else 1
current_page = st.number_input(
    f"ãƒšãƒ¼ã‚¸ï¼ˆå…¨{total_pages}ãƒšãƒ¼ã‚¸ï¼‰",
    min_value=1,
    max_value=total_pages,
    value=1,
    step=1
)

start_idx = (current_page - 1) * items_per_page
end_idx = start_idx + items_per_page
page_articles = filtered_articles[start_idx:end_idx]

# --- è¨˜äº‹ä¸€è¦§è¡¨ç¤º ---
if not page_articles:
    st.info("æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
else:
    for i, article in enumerate(page_articles, start=start_idx + 1):
        with st.expander(
            f"**{i}. {article.get('title', '(ã‚¿ã‚¤ãƒˆãƒ«ãªã—)')}** | "
            f"ä¿¡é ¼åº¦: {article.get('confidence_score', 0):.2f} | "
            f"{article.get('source', 'ä¸æ˜')} | "
            f"{article.get('published_date', 'ä¸æ˜')}"
        ):
            # 2åˆ—ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
            col_detail_left, col_detail_right = st.columns([2, 1])

            with col_detail_left:
                # è¦ç´„
                st.markdown("### ğŸ“ è¦ç´„")
                st.write(article.get('summary_japanese', 'è¦ç´„ãªã—'))

                # é‡è¦ãƒã‚¤ãƒ³ãƒˆ
                st.markdown("### ğŸ”‘ é‡è¦ãƒã‚¤ãƒ³ãƒˆ")
                for point in article.get('key_points', []):
                    st.markdown(f"- {point}")

                # è£½é€ æ¥­ã¨ã®é–¢é€£æ€§
                if article.get('manufacturing_relevance') == 'ã‚ã‚Š':
                    st.markdown("### ğŸ­ è£½é€ æ¥­ã¨ã®é–¢é€£æ€§")
                    st.info(article.get('relevance_reason', 'è¨˜è¼‰ãªã—'))

            with col_detail_right:
                # ãƒ¡ã‚¿æƒ…å ±
                st.markdown("### ğŸ“Œ è¨˜äº‹æƒ…å ±")
                st.write(f"**æƒ…å ±æº**: {article.get('source', 'ä¸æ˜')}")
                st.write(f"**å…¬é–‹æ—¥**: {article.get('published_date', 'ä¸æ˜')}")
                st.write(f"**ãƒ¬ãƒãƒ¼ãƒˆæ—¥**: {article.get('report_date', 'ä¸æ˜')}")
                st.write(f"**åœ°åŸŸ**: {article.get('region', 'ä¸æ˜')}")
                st.write(f"**ã‚«ãƒ†ã‚´ãƒª**: {article.get('category', 'ä¸æ˜')}")
                st.write(f"**è£½é€ æ¥­é–¢é€£**: {article.get('manufacturing_relevance', 'ä¸æ˜')}")
                st.write(f"**ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢**: {article.get('confidence_score', 0):.2f}")

                # é–¢é€£ä¼æ¥­
                if article.get('related_companies'):
                    st.markdown("### ğŸ¢ é–¢é€£ä¼æ¥­")
                    for company in article['related_companies']:
                        st.markdown(f"- {company}")

                # ã‚¿ã‚°
                if article.get('tags'):
                    st.markdown("### ğŸ·ï¸ ã‚¿ã‚°")
                    tags_display = ' '.join([f"`{tag}`" for tag in article['tags']])
                    st.markdown(tags_display)

                # URLãƒªãƒ³ã‚¯
                st.markdown("---")
                if article.get('url'):
                    st.markdown(f"[ğŸ”— è¨˜äº‹ã‚’é–‹ã]({article['url']})")

# ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä¸‹éƒ¨ï¼‰
if total_pages > 1:
    st.markdown("---")
    col_prev, col_info, col_next = st.columns([1, 2, 1])

    with col_prev:
        if current_page > 1:
            if st.button("â¬…ï¸ å‰ã®ãƒšãƒ¼ã‚¸"):
                st.rerun()

    with col_info:
        st.markdown(f"<div style='text-align: center;'>ãƒšãƒ¼ã‚¸ {current_page} / {total_pages}</div>", unsafe_allow_html=True)

    with col_next:
        if current_page < total_pages:
            if st.button("æ¬¡ã®ãƒšãƒ¼ã‚¸ â¡ï¸"):
                st.rerun()

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.caption("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’èª¿æ•´ã—ã¦çµã‚Šè¾¼ã¿ãŒã§ãã¾ã™")
